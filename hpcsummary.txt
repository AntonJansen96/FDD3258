Organizational ################################################################

login
    kinit --forwardable ajansen@NADA.KTH.SE         # Create Kerberos ticket.
    ssh -X ajansen@beskow.pdc.kth.se                # Login.
data
    Two file systems: AFS (Andrew File System) and Lustre.
    You login to AFS, but all files for Beskow should be stored on Lustre.

    Upon login, you are at:     /afs/pdc.kth.se/home/a/ajansen
                                Store backups of important files here.
    
    FS avail to compute nodes:  /cfs/klemming/nobackup/a/ajansen
                                /cfs/klemming/scratch/a/ajansen     (temp)
compiling/running
    salloc --nodes=1 -t 01:00:00 -A main.cc     # allocate interactive node.
    srun -n 1 ./main                            # launch program on said node.

jobscript
    #!/bin/bash

    # The name of the script is myjob
    #SBATCH -J myjob
    # Only 1 hour wall-clock time will be given to this job
    #SBATCH -t 1:00:00
    #SBATCH -A edu20.FDD3258
    # Number of nodes
    #SBATCH --nodes=1
    #SBATCH -e error_file.e

    # Run the executable file 
    # and write the output into my_output_file
    srun -n 1 ./hello.out > hello_output    

Module1 ########################################################################

benchmarks
    Slides discuss some scientific problems often used to bench clusters.
    Not relevant to me.
modeling sparse matrix-vectory multiplication
    Clock rate-based performance analysis often not useful.
    Cache design important.
    Still nothing really relevant.
more on cache memories
    Cache memory contains a copy of the main memory data.
    Cache memory is faster but consumes more space and power.
    Cache items are accessed by their address in main memory.

    memory locality
        L1      few cycles
        L2      ~ 10 cycles
        RAM     ~ 250 cycles
        Remote  ~2500-5000 cycles

    Data moves between RAM and cache in groups (chunks) called CACHE LINES.
    Size typically 64-128 bytes.
measuring performance
    All about clocks a la my C++ class.
matrix transpose
    Example of cache related performance effects using a matrix transpose.

more on caches II and III (optional)
    If a load/store from CPU cannot be satisfied from cache, a CACHE MISS occurs.

    1.  compulsory      on first access to a cache, block must be brought into 
                        cache (also called a cold start or first reference miss).
    2.  capacity        occur because lines are being discarded.
    3.  conflict        several lines are mapped to the same set (collision miss).

    Cache misses can be monitored using the perf tool.
    Perf is a performance monitoring and analyzing tool in linux.
    
instruction execution and pipelining
    About pipelining (concurrent execution of instructions)

vectorization // good
    -Vectorization is a compiler optimization, making use of CPU parallelism.
    -Vectors operator on 128 bit (16 byte) operands:
        -4 floats/ints.
        -2 doubles.
    -For g++, use "-O2 -ftree-vectorize" or simply "-O3".
    -To check, use "-fopt-info-vec -fopt-info-vec-missed".

Module2 ########################################################################

discussing speed-up
    -For parallel applications, speedup is defined as T_1/T_n.
     T_1 is time using one core etc.
    -Speedup CAN be greater then 1 (superlinear) in certain case.
    -Amdahl's law.
Moore's law and speed-up
    Usually cast as X doubles every 18-24 months.
        X:  Computer performance
            CPU Clock speed
            The number of transitors per chip at constant cost

    (end of) Dennard scaling
        -Explains why clock speeds have not increased with shrinking transistors.
        -As transistors get smaller, power density increases because these don't
         scale with size --> creating of a "power wall" (since ~2006).
threads
    A thread is a basic unit of processor utilization.
  
        -Within a process, all memory shared.
        -Each "thread" executes "normal" code.

        Can be:
            -Library-based (invoke a routine in a separate thread) (pthreads).
            -Separate enhancements to existing languages (OpenMP, CUDA).
            -Within the language itself (Java, C11).

        How to fix synchronization issues:
            -Need to impose order of memory updates
                -OpenMP has FLUSH
                -Memory barriers

            -Often, need to ensure updates happen atomically (all or nothing).

        Limits to performance
            -Threads SHARE memory resources.
            -Therefore, performance is roughly linear until bandwith is saturated.

OpenMP
    Open Multi-Processing (OpenMP) is an API that supports shared-memory
    multiprocessing programming in C/C++. It is mostly a compiler technology.
    
    library functions (#include <omp.h>)
        omp_set_num_threads()           modify/check threads
        omp_get_thread_num()            modify/check threads
        omp_get_num_threads()           modify/check threads
        omp_get_max_threads()           modify/check threads
        omp_in_parallel()               check if in parallel
        omp_set_dynamic()               dynamically vary #threads
        omp_get_dynamic()               dynamically vary #threads
        omp_num_procs()                 check #processes

    environment variables (bash)        
        OMP_NUM_THREADS                 set default #threads
        OMP_PROC_BIND                   set process binding true | false
        OMP_SCHEDULE                    Control oop iterations
        export <var> = <val>            To set an env var (in bash)

    parallel_regions
        #pragma omp parallel
        {
            // ... code executed by each thread
            
            #pragma omp [master | single]   // optional
            {
                // ... code only executed by ONE thread in parallel
            }                    
        }

        -Use "private" clause to create thread-private versions of globals.
        -Loop index variables of parallel loops are private by default.

    parallel_loops
        #pragma omp parallel            #pragma omp parallel for
        {                               {
            #pragma omp for                 // ... loop
            {                           }
                // ... loop
            }
        }

        scheduling
            The OpenMP runtime decides how the loop iterations are scheduled.
            
            static      Defined at compile-time.
            dynamic     Defined at run-time.
            guided      Special case of dynamic. Attempts to reduce overhead.

            #pragma omp parallel for schedule(kind[,chunksize])
            {
                // ... loop
            }

        reductions
            If a e.g. a parallel sum is performed, all threads will access the sum.
            However, addition is not ATOMIC, meaning we can have RACE CONDITIONS.
            This problem can be prvented by using reductions: you indicate that a
            variable is used for a "reduction" with a particular operator.

            #pragma omp parallel for reduction(+, sum)
            {
                for (int i = 0; i != j; ++i)
                    sum += a[i] * b[i]
            }

    atomic updates
        A way of coordinating access to shared values.
        
        #pragma omp atomic
        {
            // Only one threat AT A TIME can execute the following
            // STATEMENT (not block).
        }

        #pragma omp critical
        {
            // Only one thread at a time can execute the following BLOCK.
        }

        Critical sections are costly because they often require reading/writing
        to memory, and threads serialize when they are waiting for the "unlock".
        This serialization overhead will be proportional to #threads.
        Solution:   -Always try to remove dependency between shared variables.
                    --> Use a temporary array indexed by thread number to hold
                        the values found by each thread!

    false_sharing
        False sharing is an undesirable low-level sharing of variables due to
        multiple threads writing on the same chache line, causing unnecessary
        cache-misses. This can be prevented by:

        -Storing a thread-local result and only updating AFTER computation is done.
        -Spacing contigous shared memory blocks (padding).
         (e.g. typedef struct {double val, int loc; char pad[128];} tvals)
